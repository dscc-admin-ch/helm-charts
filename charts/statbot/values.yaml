api:
  image: "dsccadminch/statbot-api"
  tag: "latest"
  env:
    MODEL_PATH: "" 
    DB_HOST: ""
    DB_PORT: ""
    DB_SCHEMA: ""
    DB_USERNAME: ""
    DB_PASS: ""
    DB_DATABASE: ""

ui:
  image: "dsccadminch/statbot-ui"
  tag: "latest"
  env:
    ADMIN_MAIL: "admin@example.com"

service:
  type: NodePort
  apiPort: 5000

ingress:
    enabled: true
    hostname: "statbot.lab.sspcloud.fr"

vllm:
  nameOverride: ""
  fullnameOverride: "llm-serving"
  podAnnotations: {}

  deployment:
    image:
      repository: vllm/vllm-openai
      pullPolicy: Always
      tag: latest
    hftoken: ton_token
    args:
      model: meta-llama/Llama-3.2-3B-Instruct
      memoryutilization: 0.8
      dtype: half
      maxModelLen: 8208
    gpu:
      number: 1

  service:
    port:
      number: 8000


  ingress:
      enabled: true
      className: "nginx"
      annotations:
        nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      hostname: "llm-serving-statbot.lab.sspcloud.fr"

  s3:
    enabled: false # Set to true to use S3
    bucket: "your_bucket_name"
    modelPath: "path_to_model"
    # If not set and create is true, a name is generated using the fullname template
    accessKeyId: ""
    endpoint: ""
    defaultRegion: ""
    secretAccessKey: ""
    sessionToken: ""
