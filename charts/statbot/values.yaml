api:
  image: "dsccadminch/statbot-api"
  tag: "latest"
  env:
    MODEL_PATH: "" 
    DB_HOST: ""
    DB_PORT: ""
    DB_SCHEMA: ""
    DB_USERNAME: ""
    DB_PASS: ""
    DB_DATABASE: ""

ui:
  image: "dsccadminch/statbot-ui"
  tag: "latest"
  env:
    ADMIN_MAIL: "admin@example.com"

service:
  type: NodePort
  apiPort: 5000

ingress:
    enabled: true
    hostname: "statbot.lab.sspcloud.fr"

vllm:
  global:
    suspend: false # Allow scaling the app to 0

  enabled: true
  nameOverride: ""
  fullnameOverride: "llm-serving"
  podAnnotations: {}

  service:
    type: ClusterIP
    nodePort:
    image:
      version: "vllm/vllm-openai:v0.8.2"
      pullPolicy: IfNotPresent
      custom:
        enabled: false
        version: ""

  resources:
    limits:
      gpu:
        number: 1

  huggingFace:
    hfToken: ""

  llm:
    hfModelName: meta-llama/Llama-3.2-1B-Instruct
    localPath: "/root/.cache/huggingface"
    args: '--gpu-memory-utilization "0.8" --dtype "half" --max-model-len "8200"'

  networking:
    port:
      number: 8000

  ingress:
    enabled: true
    className: ""
    annotations:
      nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    hostname: "llm-serving-statbot.lab.sspcloud.fr"

  s3:
    enabled: true # Set to true to use S3
    s3ModelPath: ""
    accessKeyId: ""
    endpoint: ""
    defaultRegion: ""
    secretAccessKey: ""
    sessionToken: ""

  nodeSelector: {}

deployedllm:
  address: ""
  accessToken: ""